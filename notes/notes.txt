# 02/08/2023

Clear Skies Meeting #1
----------------------
Questions:
- Scientific
    - What are the N_co_adds = 50 co-adds for each u' data point?
    - What is the origin of this fast flaring?
    - How does poission uncertainties lead to white noise?

- Project
    - What do you think the timeline for the project will look like?

        - One of the things mentioned was that it would be good to have a model
          for the air pollution metric, then we could see if it significantly
          above or below the model.

    - What are the key milestones?
    - I will keep a continuous monolithic journal for the days I work on the
      project starting today.

- Administrative
    - Shall I reach out to Dr Nicholas Marsden at Manchester for data.
    - Will I be paid restroactively?

- Logistical
    - How often should we have meeting be?
    - How can we best make this project a success?
    - What is your prefered method of supervision?


Manchester Air Quality Supersite Time Resolution
------------------------------------------------
Gases:
1  Second: Ammonia (NH3), Carbon Dioxide (CO2), Methane (CH4)
10 Second: Nitrogen Dioxide (NO_2)
20 Second: Ozone (O_3), Nitric Oxide (NO), (NO_y)

Aerosols:
1  Second: Particle Number
1  Minute: Black/Brown Carbon, Particulate Matter (PM1, PM2.5, PM10)
5  Minute: Particle Size Distribution
30 Minute: Non-refractory composition
1  Hour:   Elemental Composition

Meterology:
20Hz (50 ms) : Wind Speed and Direction
5  Second    : Solar radiation
15 Second    : Boundary Layer and Cloud Base Height
1  Minute    : Precipitation (rate and type), Temperature, Humidity and Pressure


NO_x is NOx is shorthand for nitric oxide (NO) and nitrogen dioxide (NO_2)
NOx does not include nitrous oxide (N_2 O)

NOy is defined as the sum of NOx plus the NOz compounds produced from the
oxidation of NOx which include nitric acid, nitrous acid (HONO), dinitrogen
pentoxide (N2O5), peroxyacetyl nitrate (PAN), alkyl nitrates (RONO2),
peroxyalkyl nitrates (ROONO2), the nitrate radical (NO3), and peroxynitric acid
(HNO4). See: https://en.wikipedia.org/wiki/NOx


I have downloaded some data from the UK Air Information Data Archive for Southampton
Center and also Tower Hamlets.

The data has a frequency of 1hr intervals and contains NaN values that could
realisitically be interpolated.

NO, NO2 and NOXasNO2 are all strongly correlated
PM10 and PM25 are also strongly correlated.

NO and SO2 are correlated at r~0.6


[x] Download Data from UK Air Information Data Archive
[x] Create correlation matricies and corner plots for the data 
[x] Create PSDs from the data


# 03/08/2023

A comon method of analysing PSDs in astronomy is by decomposing the spectrum into
a summation of Lorentzian components.

For example in GX339-4 it is found that three Lorentzian components are capable of
describing the spectrum.

Fitting a single Lorentzian to the data is fairly trivial, however specifying
an arbitrary number of fit variables using scipy.optimize becomes rather messy
very quickly as the parameters must be processed in the function by their index
eg. params[0], and then then must be collected up at the end while keeping
track of the order. Although this is doable, the lmfit provides a provides a
high-level interface to non-linear optimization and curve fitting problems for
Python. It builds and expands on many of the methods found in scipy.optimize
and seems suitable for this paticular problem.


# 04/08/2023

I have managed to create some data created from two summed Lorentzians and use lmfit
to fit the data to reproduce the original parameters from the component model.

Now I need to see how well this works for the PSD, my suspicion is that I will likely
have to perform some data binning on the PSD in order for the fit to converge well. I
also think I will have to perform some normalization on my PSDs, as currently they
are topping out at 1e9 and other extremely large values.

I have added in the normalization to the PSD to follow the usual rms-squared standardization,
this has significantly brought down the maximum values reached by the PSD.

I have binned the data using the following method:
    - Createa linspace() of the frequency range with N_bins+1 values.
    - Find the centers of each bin by finding the halfway points between each value in the array
    - use np.digitize to get which bin index each value falls into for example:
            psd[0] falls into bin [1]
            psd[27] falls bin[4]
            returned array: [1,1,1,2,2,2,3,3,3,4...]
    - Get the mean of each bin by looping over all bins (1, N_bins+1)
    for each bin, filter the values in that bin and calculate the mean

    x_bin_edges   = [0.0000 0.010 0.020  0.030 0.040]
    x_bin_centers = [0.0050 0.015 0.025  0.035 0.045]
    x_bin_indices = [1      1     1      1     1]
    bin_means     = [1.35  0.014 0.0086 0.0088 0.043]

[x] Normalize PSDs

# 07/08/2023

I will now attempt to fit the N Lorentzian model to the binned data, one thing
I have been unsure about until now is how exactly the fitting will work with
the data, the PSDs are normally plotted in Log-Log space othwise it is
impossible to see any of the detail in the PSD, however I don't know if fitting
to the data in linear space will be successful or not, or whether I should
convert the model to output log10() of its normal value and then also log10()
the data and do the fit there.

I imagine there is likely some sublety involved in the minimization routine
depending on the spacing of the data points, I will investigate today.

I have also now created a python package called "pysmog" that I will iterate upon
as I add more analysis and data processing to the pipeline.

[x] Create Python package
[x] Arrange Meeting with Poshak

# 08/08/2023
[x] Read through and take notes on GX399-4 Paper.

I have performed a cross-correlation on the various different combinations of parameters
in the initial dataset I obtained from airqualityengland.co.uk.

Many features appear uncorrelated across time with symetrical triangular
cross-correlation functions, some appear to have asymetrical profiles with respect
to 0 lag, suggesting the presence of long-term trends.

Generally, it is not possible to determine causality from an
arbitrary shape of the cross-correlation function. However, if the
cross-correlation function is asymmetric with respect to the time
reversal operation (change of a sign of the time lag), it might hint at
the presence of causal relationships.

see: docs/1977_Pierce_Causality_in_temporal_systems.pdf

Similar features such as NO vs NO2 and PM10 vs PM25 are strongly positively
correlated with lags of 0 as is to be expected, however, the analysis reveals
that there may be other interesting relations between parameters.

    - PM10 vs NO
    - 

By continually moving code from jupyter notebooks into the pysmog module
allowing me to keep the jupyter notebooks cleaner as well as more consisntent over
time.

[x] Create cross correlations

# 09/08/2023

Meeting 10am
    
    - Ask about interpreation of cross-correlation functions
        - Some averaging of the CCFs could be done to better constrain the shapes
    - Fitting Lorentzians in log vs linear space
        - It could be that the function and the frequencies should be converted to log.
    - Advice for Postdoc interview
        - Be yourself
    - Ask about RMS-Flux relationship 
        - 
    - Ask about the baseline modelling idea

    Spent most of today going on a long cycle to Layton so didn't get much done on the work front



# 14/08/2023
Spent aa bit of time now trying to do some averaging of the CCFs and PSDs.

# 22/08/2023 

Had interview for Tolouse this morning, think it went quite well, I worked a
bit more on the lmfit routine to fit the PSDs, one issue is adding the
Lorentzian components in log-space results in incorrect values.


        
# TODO (x:done -:started)
[ ] Create Lorentzian fits to PSD
[ ] Work on a routine to calculate the RMS-Flux relationship
[ ] Create Averaged PSDs
[ ] Created Averaged CCFs
[ ] Think about creating tests for the module.
